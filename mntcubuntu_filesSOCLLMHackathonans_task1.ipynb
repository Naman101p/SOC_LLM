{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.356462</td>\n",
       "      <td>0.536762</td>\n",
       "      <td>-0.082446</td>\n",
       "      <td>-0.015523</td>\n",
       "      <td>0.095265</td>\n",
       "      <td>0.266100</td>\n",
       "      <td>0.108517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>-0.356462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.150826</td>\n",
       "      <td>-0.365902</td>\n",
       "      <td>0.065187</td>\n",
       "      <td>0.023666</td>\n",
       "      <td>-0.552893</td>\n",
       "      <td>-0.108502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <td>0.536762</td>\n",
       "      <td>-0.150826</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.099037</td>\n",
       "      <td>0.106296</td>\n",
       "      <td>0.249543</td>\n",
       "      <td>0.182457</td>\n",
       "      <td>0.097129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>-0.082446</td>\n",
       "      <td>-0.365902</td>\n",
       "      <td>-0.099037</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.307351</td>\n",
       "      <td>-0.187896</td>\n",
       "      <td>0.093143</td>\n",
       "      <td>0.012186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <td>-0.015523</td>\n",
       "      <td>0.065187</td>\n",
       "      <td>0.106296</td>\n",
       "      <td>-0.307351</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.383338</td>\n",
       "      <td>0.139860</td>\n",
       "      <td>0.004021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <td>0.095265</td>\n",
       "      <td>0.023666</td>\n",
       "      <td>0.249543</td>\n",
       "      <td>-0.187896</td>\n",
       "      <td>0.383338</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.206624</td>\n",
       "      <td>-0.014082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>0.266100</td>\n",
       "      <td>-0.552893</td>\n",
       "      <td>0.182457</td>\n",
       "      <td>0.093143</td>\n",
       "      <td>0.139860</td>\n",
       "      <td>0.206624</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.176859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked</th>\n",
       "      <td>0.108517</td>\n",
       "      <td>-0.108502</td>\n",
       "      <td>0.097129</td>\n",
       "      <td>0.012186</td>\n",
       "      <td>0.004021</td>\n",
       "      <td>-0.014082</td>\n",
       "      <td>0.176859</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Survived    Pclass       Sex       Age     SibSp     Parch  \\\n",
       "Survived  1.000000 -0.356462  0.536762 -0.082446 -0.015523  0.095265   \n",
       "Pclass   -0.356462  1.000000 -0.150826 -0.365902  0.065187  0.023666   \n",
       "Sex       0.536762 -0.150826  1.000000 -0.099037  0.106296  0.249543   \n",
       "Age      -0.082446 -0.365902 -0.099037  1.000000 -0.307351 -0.187896   \n",
       "SibSp    -0.015523  0.065187  0.106296 -0.307351  1.000000  0.383338   \n",
       "Parch     0.095265  0.023666  0.249543 -0.187896  0.383338  1.000000   \n",
       "Fare      0.266100 -0.552893  0.182457  0.093143  0.139860  0.206624   \n",
       "Embarked  0.108517 -0.108502  0.097129  0.012186  0.004021 -0.014082   \n",
       "\n",
       "              Fare  Embarked  \n",
       "Survived  0.266100  0.108517  \n",
       "Pclass   -0.552893 -0.108502  \n",
       "Sex       0.182457  0.097129  \n",
       "Age       0.093143  0.012186  \n",
       "SibSp     0.139860  0.004021  \n",
       "Parch     0.206624 -0.014082  \n",
       "Fare      1.000000  0.176859  \n",
       "Embarked  0.176859  1.000000  "
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"./LLM/Hackathon/train.csv\", usecols= [ \"Survived\" , \"Pclass\", \"Sex\" , \"Age\",\"SibSp\" , \"Parch\",\"Fare\",\"Embarked\"]).dropna()\n",
    "test_data = pd.read_csv(\"./LLM/Hackathon/train.csv\", usecols= [ \"Survived\" , \"Pclass\", \"Sex\" , \"Age\",\"SibSp\" , \"Parch\",\"Fare\",\"Embarked\"]).dropna()\n",
    "# os.system('pwd')\n",
    "train_data[\"Sex\"] = train_data[\"Sex\"].replace(\"male\" , 0)\n",
    "train_data[\"Sex\"] = train_data[\"Sex\"].replace(\"female\" , 1)\n",
    "test_data[\"Sex\"] = test_data[\"Sex\"].replace(\"male\" , 0)\n",
    "test_data[\"Sex\"] = test_data[\"Sex\"].replace(\"female\" , 1)\n",
    "train_data[\"Embarked\"] = train_data[\"Embarked\"].replace(\"S\" , 0)\n",
    "train_data[\"Embarked\"] = train_data[\"Embarked\"].replace(\"C\" , 1)\n",
    "train_data[\"Embarked\"] = train_data[\"Embarked\"].replace(\"Q\" , 2)\n",
    "test_data[\"Embarked\"] = test_data[\"Embarked\"].replace(\"S\" , 0)\n",
    "test_data[\"Embarked\"] = test_data[\"Embarked\"].replace(\"C\" , 1)\n",
    "test_data[\"Embarked\"] = test_data[\"Embarked\"].replace(\"Q\" , 2)\n",
    "\n",
    "# train_data = train_data.replace(\"nan\",0)\n",
    "# train_data.fillna(0)\n",
    "# test_data.fillna(0)\n",
    "# print(train_data[\"Age\"][888])\n",
    "train_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Embarked\n",
       "0         0       3    0  22.0      1      0   7.2500         0\n",
       "1         1       1    1  38.0      1      0  71.2833         1\n",
       "2         1       3    1  26.0      0      0   7.9250         0\n",
       "3         1       1    1  35.0      1      0  53.1000         0\n",
       "4         0       3    0  35.0      0      0   8.0500         0"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = train_data.max(axis= 0)\n",
    "min_val = train_data.min(axis= 0)\n",
    " \n",
    "range = max_val - min_val\n",
    "train_data = (train_data - min_val)/(range)\n",
    " \n",
    "test_data =  (test_data- min_val)/range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Pclass  Sex   Age  SibSp  Parch     Fare  Embarked\n",
      "0         3    0  22.0      1      0   7.2500         0\n",
      "1         1    1  38.0      1      0  71.2833         1\n",
      "2         3    1  26.0      0      0   7.9250         0\n",
      "3         1    1  35.0      1      0  53.1000         0\n",
      "4         3    0  35.0      0      0   8.0500         0\n",
      "..      ...  ...   ...    ...    ...      ...       ...\n",
      "885       3    1  39.0      0      5  29.1250         2\n",
      "886       2    0  27.0      0      0  13.0000         0\n",
      "887       1    1  19.0      0      0  30.0000         0\n",
      "889       1    0  26.0      0      0  30.0000         1\n",
      "890       3    0  32.0      0      0   7.7500         2\n",
      "\n",
      "[712 rows x 7 columns]\n",
      "0      0\n",
      "1      1\n",
      "2      1\n",
      "3      1\n",
      "4      0\n",
      "      ..\n",
      "885    0\n",
      "886    0\n",
      "887    1\n",
      "889    1\n",
      "890    0\n",
      "Name: Survived, Length: 712, dtype: int64\n",
      "(712, 7)\n",
      "[7]\n"
     ]
    }
   ],
   "source": [
    "X_train = train_data.drop(\"Survived\",axis=1)\n",
    "y_train = train_data[\"Survived\"]\n",
    "X_val = test_data.drop(\"Survived\",axis=1)\n",
    "Y_val = test_data[\"Survived\"]\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "print(X_train.shape)\n",
    "# X_train = np.asarray(X_train).astype(np.float32)\n",
    "# y_train = np.asarray(y_train).astype(np.float32)\n",
    "# X_val = np.asarray(X_val).astype(np.float32)\n",
    "# Y_val = np.asarray(Y_val).astype(np.float32)\n",
    "input_shape = [X_train.shape[1]]\n",
    "print(input_shape)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_70 (Dense)            (None, 20)                160       \n",
      "                                                                 \n",
      " dense_71 (Dense)            (None, 20)                420       \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 601\n",
      "Trainable params: 601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    " \n",
    "    tf.keras.layers.Dense(units=20, activation='relu',\n",
    "                          input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(units=20, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1,activation='sigmoid')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "               \n",
    "              # MAE error is good for\n",
    "              # numerical predictions\n",
    "              loss= tf.keras.losses.BinaryCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      "72/72 [==============================] - 2s 7ms/step - loss: 0.6780 - val_loss: 0.6179\n",
      "Epoch 2/9\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.5710 - val_loss: 0.5254\n",
      "Epoch 3/9\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.4949 - val_loss: 0.4691\n",
      "Epoch 4/9\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 0.4646 - val_loss: 0.4533\n",
      "Epoch 5/9\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.4548 - val_loss: 0.4470\n",
      "Epoch 6/9\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 0.4485 - val_loss: 0.4433\n",
      "Epoch 7/9\n",
      "72/72 [==============================] - 0s 7ms/step - loss: 0.4447 - val_loss: 0.4421\n",
      "Epoch 8/9\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.4422 - val_loss: 0.4383\n",
      "Epoch 9/9\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.4437 - val_loss: 0.4361\n"
     ]
    }
   ],
   "source": [
    "losses = model.fit(X_train, y_train,\n",
    " \n",
    "                   validation_data=(X_val, Y_val),\n",
    "                    \n",
    "                   # it will use 'batch_size' number\n",
    "                   # of examples per example\n",
    "                   batch_size=10,\n",
    "                   epochs=9,  # total epoch\n",
    " \n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 7) <dtype: 'float32'>\n",
      "(None, 1) <dtype: 'float32'>\n",
      "dense_25 (None, 7) float32\n",
      "dense_26 (None, 20) float32\n",
      "dense_27 (None, 20) float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(i.shape, i.dtype) for i in model.inputs]\n",
    "[print(o.shape, o.dtype) for o in model.outputs]\n",
    "[print(l.name, l.input_shape, l.dtype) for l in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense_Layer(tf.keras.layers.Layer):\n",
    "\n",
    "     def __init__(self,units):\n",
    "        super(Dense_Layer,self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "     def build(self,input_shape):\n",
    "        self.w1 = self.add_weight(shape=(input_shape[1],self.units),initializer='random_normal',trainable = True)\n",
    "        self.w2 = self.add_weight(shape=(self.units,1),initializer='random_normal',trainable = True)\n",
    "        self.b = self.add_weight(shape=(self.units,1),initializer='random_normal',trainable = True)\n",
    "    \n",
    "     def call(self,inputs):\n",
    "        return tf.transpose(tf.multiply(tf.linalg.matmul(tf.transpose(self.w1),tf.transpose(inputs)),self.w2) + self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Model(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Custom_Model, self).__init__()\n",
    "        self.dense1 = Dense_Layer(10)\n",
    "        self.dense2 = Dense_Layer(20)\n",
    "        self.dense3  = Dense_Layer(1)\n",
    "       \n",
    "    def call(self,input_tensor,training=False):\n",
    "\n",
    "        x = self.dense1(input_tensor)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.dense3(x)\n",
    "        x = tf.nn.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_func(model,x,y,loss):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)                           # Model's prediction\n",
    "        loss_value = loss(y, logits)             # Calcuate Loss\n",
    "\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)  # Calculate Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "model = Custom_Model()\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "train_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "print(model.trainable_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_inp,val_inp,train_out,val_out = train_test_split(X_train,y_train,test_size=0.2,random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_inp, train_out))\n",
    "train_data = train_data.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_inp,val_out))\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Training Loss At step 0: 0.5041521787643433\n",
      "Training Loss At step 1: 0.53453528881073\n",
      "Training Loss At step 2: 0.4366776943206787\n",
      "Training Loss At step 3: 0.6354720592498779\n",
      "Training Loss At step 4: 0.7063269019126892\n",
      "Training Loss At step 5: 0.775420069694519\n",
      "Training Loss At step 6: 0.5754715204238892\n",
      "Training Loss At step 7: 0.4847244918346405\n",
      "Training Loss At step 8: 0.5570775270462036\n",
      "Training Loss At step 9: 0.48543018102645874\n",
      "Training Loss At step 10: 0.6325913667678833\n",
      "Training Loss At step 11: 0.5552159547805786\n",
      "Training Loss At step 12: 0.6964458227157593\n",
      "Training Loss At step 13: 0.45940542221069336\n",
      "Training Loss At step 14: 0.47464561462402344\n",
      "Training Loss At step 15: 0.583739161491394\n",
      "Training Loss At step 16: 0.5448042154312134\n",
      "Training Loss At step 17: 0.5551363229751587\n",
      "Training Loss At step 18: 0.5460162162780762\n",
      "Training Loss At step 19: 0.385837197303772\n",
      "Training Loss At step 20: 0.8045790195465088\n",
      "Training Loss At step 21: 0.8205664157867432\n",
      "Training Loss At step 22: 0.5232899785041809\n",
      "Training Loss At step 23: 0.5388431549072266\n",
      "Training Loss At step 24: 0.41462188959121704\n",
      "Training Loss At step 25: 0.7476178407669067\n",
      "Training Loss At step 26: 0.41069093346595764\n",
      "Training Loss At step 27: 0.7674280405044556\n",
      "Training Loss At step 28: 0.7127663493156433\n",
      "Training Loss At step 29: 0.4280751943588257\n",
      "Training Loss At step 30: 0.5656372308731079\n",
      "Training Loss At step 31: 0.45296651124954224\n",
      "Training Loss At step 32: 0.4587845206260681\n",
      "Training Loss At step 33: 0.6161785125732422\n",
      "Training Loss At step 34: 0.46506041288375854\n",
      "Training Loss At step 35: 0.7842548489570618\n",
      "Training Accuracy:0.6959578394889832\n",
      "Validation Accuracy:0.7622377872467041\n",
      "2\n",
      "Training Loss At step 0: 0.3683730959892273\n",
      "Training Loss At step 1: 0.48309728503227234\n",
      "Training Loss At step 2: 0.588151216506958\n",
      "Training Loss At step 3: 0.422598659992218\n",
      "Training Loss At step 4: 0.45904985070228577\n",
      "Training Loss At step 5: 0.44822973012924194\n",
      "Training Loss At step 6: 0.4984012842178345\n",
      "Training Loss At step 7: 0.48349612951278687\n",
      "Training Loss At step 8: 0.5648907423019409\n",
      "Training Loss At step 9: 0.35471072793006897\n",
      "Training Loss At step 10: 0.48871949315071106\n",
      "Training Loss At step 11: 0.3281853199005127\n",
      "Training Loss At step 12: 0.6637773513793945\n",
      "Training Loss At step 13: 0.5564084053039551\n",
      "Training Loss At step 14: 0.5550307631492615\n",
      "Training Loss At step 15: 0.5665510296821594\n",
      "Training Loss At step 16: 0.6642311811447144\n",
      "Training Loss At step 17: 0.7053306698799133\n",
      "Training Loss At step 18: 0.3510333299636841\n",
      "Training Loss At step 19: 0.5175763368606567\n",
      "Training Loss At step 20: 0.5844786763191223\n",
      "Training Loss At step 21: 0.4521111845970154\n",
      "Training Loss At step 22: 0.42965567111968994\n",
      "Training Loss At step 23: 0.5253722667694092\n",
      "Training Loss At step 24: 0.3722992539405823\n",
      "Training Loss At step 25: 0.7075980305671692\n",
      "Training Loss At step 26: 0.44993293285369873\n",
      "Training Loss At step 27: 0.5511727929115295\n",
      "Training Loss At step 28: 0.5937374830245972\n",
      "Training Loss At step 29: 0.390949547290802\n",
      "Training Loss At step 30: 0.5642445087432861\n",
      "Training Loss At step 31: 0.5436619520187378\n",
      "Training Loss At step 32: 0.7531620264053345\n",
      "Training Loss At step 33: 0.4444581866264343\n",
      "Training Loss At step 34: 0.757296085357666\n",
      "Training Loss At step 35: 0.43109041452407837\n",
      "Training Accuracy:0.7592267394065857\n",
      "Validation Accuracy:0.7692307829856873\n",
      "3\n",
      "Training Loss At step 0: 0.5093531608581543\n",
      "Training Loss At step 1: 0.519201397895813\n",
      "Training Loss At step 2: 0.6230969429016113\n",
      "Training Loss At step 3: 0.44290459156036377\n",
      "Training Loss At step 4: 0.4677964448928833\n",
      "Training Loss At step 5: 0.5644147992134094\n",
      "Training Loss At step 6: 0.6561858057975769\n",
      "Training Loss At step 7: 0.36990413069725037\n",
      "Training Loss At step 8: 0.6358888149261475\n",
      "Training Loss At step 9: 0.4884193241596222\n",
      "Training Loss At step 10: 0.5844350457191467\n",
      "Training Loss At step 11: 0.505379855632782\n",
      "Training Loss At step 12: 0.4771578311920166\n",
      "Training Loss At step 13: 0.40829700231552124\n",
      "Training Loss At step 14: 0.4906424880027771\n",
      "Training Loss At step 15: 0.7478506565093994\n",
      "Training Loss At step 16: 0.5075478553771973\n",
      "Training Loss At step 17: 0.7995995283126831\n",
      "Training Loss At step 18: 0.644878625869751\n",
      "Training Loss At step 19: 0.5267762541770935\n",
      "Training Loss At step 20: 0.5308141708374023\n",
      "Training Loss At step 21: 0.5768367052078247\n",
      "Training Loss At step 22: 0.5499742031097412\n",
      "Training Loss At step 23: 0.23501726984977722\n",
      "Training Loss At step 24: 0.5809847712516785\n",
      "Training Loss At step 25: 0.6885946393013\n",
      "Training Loss At step 26: 0.2642388641834259\n",
      "Training Loss At step 27: 0.43327590823173523\n",
      "Training Loss At step 28: 0.6358538269996643\n",
      "Training Loss At step 29: 0.3587113916873932\n",
      "Training Loss At step 30: 0.35672610998153687\n",
      "Training Loss At step 31: 0.416959285736084\n",
      "Training Loss At step 32: 0.5314368009567261\n",
      "Training Loss At step 33: 0.36482706665992737\n",
      "Training Loss At step 34: 0.48171961307525635\n",
      "Training Loss At step 35: 0.5787880420684814\n",
      "Training Accuracy:0.7697715163230896\n",
      "Validation Accuracy:0.7832167744636536\n",
      "4\n",
      "Training Loss At step 0: 0.3397046625614166\n",
      "Training Loss At step 1: 0.7435843348503113\n",
      "Training Loss At step 2: 0.38514983654022217\n",
      "Training Loss At step 3: 0.49359989166259766\n",
      "Training Loss At step 4: 0.641936719417572\n",
      "Training Loss At step 5: 0.49684053659439087\n",
      "Training Loss At step 6: 0.24974651634693146\n",
      "Training Loss At step 7: 0.6250949501991272\n",
      "Training Loss At step 8: 0.3441559970378876\n",
      "Training Loss At step 9: 0.44414591789245605\n",
      "Training Loss At step 10: 0.4649600684642792\n",
      "Training Loss At step 11: 0.47787946462631226\n",
      "Training Loss At step 12: 0.5337597131729126\n",
      "Training Loss At step 13: 0.5112566947937012\n",
      "Training Loss At step 14: 0.42547038197517395\n",
      "Training Loss At step 15: 0.5998356938362122\n",
      "Training Loss At step 16: 0.40117835998535156\n",
      "Training Loss At step 17: 0.880989670753479\n",
      "Training Loss At step 18: 0.5473411083221436\n",
      "Training Loss At step 19: 0.39666932821273804\n",
      "Training Loss At step 20: 0.3967513144016266\n",
      "Training Loss At step 21: 0.5807853937149048\n",
      "Training Loss At step 22: 0.5378285050392151\n",
      "Training Loss At step 23: 0.43822774291038513\n",
      "Training Loss At step 24: 0.48291775584220886\n",
      "Training Loss At step 25: 0.6463463306427002\n",
      "Training Loss At step 26: 0.6580767631530762\n",
      "Training Loss At step 27: 0.3835389018058777\n",
      "Training Loss At step 28: 0.36486831307411194\n",
      "Training Loss At step 29: 0.5583283305168152\n",
      "Training Loss At step 30: 0.4315851330757141\n",
      "Training Loss At step 31: 0.2857149839401245\n",
      "Training Loss At step 32: 0.5852456092834473\n",
      "Training Loss At step 33: 0.4506075978279114\n",
      "Training Loss At step 34: 0.42039012908935547\n",
      "Training Loss At step 35: 0.42973119020462036\n",
      "Training Accuracy:0.7838312983512878\n",
      "Validation Accuracy:0.7832167744636536\n",
      "5\n",
      "Training Loss At step 0: 0.4073563814163208\n",
      "Training Loss At step 1: 0.2933497428894043\n",
      "Training Loss At step 2: 0.7339215874671936\n",
      "Training Loss At step 3: 0.4422905743122101\n",
      "Training Loss At step 4: 0.3529359698295593\n",
      "Training Loss At step 5: 0.5562714338302612\n",
      "Training Loss At step 6: 0.9057682752609253\n",
      "Training Loss At step 7: 0.6970483660697937\n",
      "Training Loss At step 8: 0.282459020614624\n",
      "Training Loss At step 9: 0.4998478889465332\n",
      "Training Loss At step 10: 0.4426780045032501\n",
      "Training Loss At step 11: 0.8379682898521423\n",
      "Training Loss At step 12: 0.41422075033187866\n",
      "Training Loss At step 13: 0.45444488525390625\n",
      "Training Loss At step 14: 0.5312923789024353\n",
      "Training Loss At step 15: 0.7674359083175659\n",
      "Training Loss At step 16: 0.7440612316131592\n",
      "Training Loss At step 17: 0.3987671732902527\n",
      "Training Loss At step 18: 0.47297900915145874\n",
      "Training Loss At step 19: 0.28626853227615356\n",
      "Training Loss At step 20: 0.4337959587574005\n",
      "Training Loss At step 21: 0.5158032774925232\n",
      "Training Loss At step 22: 0.3958083987236023\n",
      "Training Loss At step 23: 0.36592912673950195\n",
      "Training Loss At step 24: 0.5998914837837219\n",
      "Training Loss At step 25: 0.45951926708221436\n",
      "Training Loss At step 26: 0.4465305805206299\n",
      "Training Loss At step 27: 0.26183485984802246\n",
      "Training Loss At step 28: 0.580588698387146\n",
      "Training Loss At step 29: 0.24724894762039185\n",
      "Training Loss At step 30: 0.5491852164268494\n",
      "Training Loss At step 31: 0.4755658507347107\n",
      "Training Loss At step 32: 0.5215879082679749\n",
      "Training Loss At step 33: 0.31824201345443726\n",
      "Training Loss At step 34: 0.4336549639701843\n",
      "Training Loss At step 35: 0.34979891777038574\n",
      "Training Accuracy:0.7873462438583374\n",
      "Validation Accuracy:0.7832167744636536\n",
      "6\n",
      "Training Loss At step 0: 0.42740702629089355\n",
      "Training Loss At step 1: 0.1989305168390274\n",
      "Training Loss At step 2: 0.41593992710113525\n",
      "Training Loss At step 3: 0.5097675919532776\n",
      "Training Loss At step 4: 0.32534706592559814\n",
      "Training Loss At step 5: 0.5712598562240601\n",
      "Training Loss At step 6: 0.4895550608634949\n",
      "Training Loss At step 7: 0.7493550181388855\n",
      "Training Loss At step 8: 0.6764246225357056\n",
      "Training Loss At step 9: 0.27319014072418213\n",
      "Training Loss At step 10: 0.5635744333267212\n",
      "Training Loss At step 11: 0.39687392115592957\n",
      "Training Loss At step 12: 0.56026291847229\n",
      "Training Loss At step 13: 0.3311901092529297\n",
      "Training Loss At step 14: 0.4457350969314575\n",
      "Training Loss At step 15: 0.8642335534095764\n",
      "Training Loss At step 16: 0.517872154712677\n",
      "Training Loss At step 17: 0.6161928772926331\n",
      "Training Loss At step 18: 0.36595407128334045\n",
      "Training Loss At step 19: 0.434950590133667\n",
      "Training Loss At step 20: 0.5638884902000427\n",
      "Training Loss At step 21: 0.4423346519470215\n",
      "Training Loss At step 22: 0.5207668542861938\n",
      "Training Loss At step 23: 0.4442983865737915\n",
      "Training Loss At step 24: 0.6712702512741089\n",
      "Training Loss At step 25: 0.24288484454154968\n",
      "Training Loss At step 26: 0.49314531683921814\n",
      "Training Loss At step 27: 0.2704106569290161\n",
      "Training Loss At step 28: 0.64251708984375\n",
      "Training Loss At step 29: 0.5224802494049072\n",
      "Training Loss At step 30: 0.43497949838638306\n",
      "Training Loss At step 31: 0.396383136510849\n",
      "Training Loss At step 32: 0.6666560173034668\n",
      "Training Loss At step 33: 0.4780869781970978\n",
      "Training Loss At step 34: 0.2637660801410675\n",
      "Training Loss At step 35: 0.8760568499565125\n",
      "Training Accuracy:0.790861189365387\n",
      "Validation Accuracy:0.7832167744636536\n"
     ]
    }
   ],
   "source": [
    "TrainingAccuracy = []\n",
    "TrainingLosses = []\n",
    "ValAccuracy = []\n",
    "lst = [1,2,3,4,5,6]\n",
    "for epoch in lst:\n",
    "    print(epoch)\n",
    "    for batch_number,(batch_X,batch_y) in enumerate(train_data):\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(batch_X,training = True)\n",
    "                loss_value = loss(batch_y,logits)\n",
    "                TrainingLosses.append(loss_value)\n",
    "                grads = tape.gradient(loss_value,model.trainable_weights)\n",
    "                optimizer.apply_gradients(zip(grads,model.trainable_weights))\n",
    "                train_acc_metric(batch_y,logits)\n",
    "                print(f\"Training Loss At step {batch_number}: {loss_value}\")\n",
    "    train_acc = train_acc_metric.result()\n",
    "    TrainingAccuracy.append(train_acc)\n",
    "    print(f\"Training Accuracy:{train_acc}\")\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "\n",
    "    for val_batch_X, val_batch_y in val_dataset:\n",
    "            val_logits = model(val_batch_X)\n",
    "            val_acc_metric(val_batch_y,val_logits)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    ValAccuracy.append(val_acc)\n",
    "    print(f\"Validation Accuracy:{val_acc}\")\n",
    "    val_acc_metric.reset_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
